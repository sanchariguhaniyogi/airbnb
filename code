Packages to be installed:
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import GradientBoostingRegressor
from pyearth import Earth

	Python codes for regression models (without log transformation):

# Load data (Replace the filename accordingly)
df = pd.read_csv(‘filename.csv’)

# Split data into input and output
X = df.drop(['realSum'], axis=1)
y = df['realSum']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create a list of models
models = [('Linear Regression', LinearRegression( )),
          ('Ridge Regression', Ridge( )),
          ('Lasso Regression', Lasso( )),
          
          ('MARS', Earth(max_degree=1, penalty=3.0, endspan=5, feature_importance_type='rss'))]

# Evaluate each model and store performance metrics and equations in a dictionary
metrics = {'Model': [ ], 'MAE': [ ], 'MSE': [ ], 'R-sq': [ ], 'Equation': [ ]}
for name, model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    metrics['Model'].append(name)
    metrics['MAE'].append(mae)
    metrics['MSE'].append(mse)
    metrics['R-sq'].append(r2)
    if name == 'MARS':
        metrics['Equation'].append(model.summary())
    else:
        coefs = model.coef_
        intercept = model.intercept_
        equation = f'{intercept:+.3f} '
        for i, coef in enumerate(coefs):
            equation += f'{coef:+.3f} X{i+1} '
        metrics['Equation'].append(equation)

# Print the performance metrics and equations in a table
print(pd.DataFrame(metrics))

	Python codes for regression models (after log normalisation):
# Split data into predictors and target variable
X = data.drop('realSum', axis=1)
y = np.log(data['realSum'])

# Take log of the large parameters (Note that at first we would only take log of prices)
#X['attr_index'] = np.log(X['attr_index'])
#X['rest_index'] = np.log(X['rest_index'])

# Split data into training and test sets
n = len(data)
train_size = int(0.7 * n)
train_X, train_y = X[:train_size], y[:train_size]
test_X, test_y = X[train_size:], y[train_size:]

# Linear Regression
lin_reg = LinearRegression()
lin_reg.fit(train_X, train_y)
lin_reg_pred = lin_reg.predict(test_X)
lin_reg_rmse = np.sqrt(mean_squared_error(test_y, lin_reg_pred))
lin_reg_mae = mean_absolute_error(test_y, lin_reg_pred)
lin_reg_r2 = r2_score(test_y, lin_reg_pred)

# Polynomial Regression
poly_reg = PolynomialFeatures(degree=2)
train_X_poly = poly_reg.fit_transform(train_X)
test_X_poly = poly_reg.transform(test_X)
poly_lin_reg = LinearRegression()
poly_lin_reg.fit(train_X_poly, train_y)
poly_lin_reg_pred = poly_lin_reg.predict(test_X_poly)
poly_lin_reg_rmse = np.sqrt(mean_squared_error(test_y, poly_lin_reg_pred))
poly_lin_reg_mae = mean_absolute_error(test_y, poly_lin_reg_pred)
poly_lin_reg_r2 = r2_score(test_y, poly_lin_reg_pred)

# Lasso Regression
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(train_X, train_y)
lasso_reg_pred = lasso_reg.predict(test_X)
lasso_reg_rmse = np.sqrt(mean_squared_error(test_y, lasso_reg_pred))
lasso_reg_mae = mean_absolute_error(test_y, lasso_reg_pred)
lasso_reg_r2 = r2_score(test_y, lasso_reg_pred)

# Ridge Regression
ridge_reg = Ridge(alpha=0.1)
ridge_reg.fit(train_X, train_y)
ridge_reg_pred = ridge_reg.predict(test_X)
ridge_reg_rmse = np.sqrt(mean_squared_error(test_y, ridge_reg_pred))
ridge_reg_mae = mean_absolute_error(test_y, ridge_reg_pred)
ridge_reg_r2 = r2_score(test_y, ridge_reg_pred)

# MARS
mars = Earth(max_degree=1, penalty=3, endspan=6, feature_importance_type='rss')
mars.fit(train_X, train_y)
mars_pred = mars.predict(test_X)
mars_rmse = np.sqrt(mean_squared_error(test_y, mars_pred))
mars_mae = mean_absolute_error(test_y, mars_pred)
mars_r2 = r2_score(test_y, mars_pred)

# Print evaluation metrics for each model
print('Linear Regression: RMSE = {:.3f}, MAE = {:.3f}, R^2 = {:.3f}'.format(lin_reg_rmse, lin_reg_mae, lin_reg_r2))
print('Polynomial Regression: RMSE = {:.3f}, MAE = {:.3f}, R^2 = {:.3f}'.format(poly_lin_reg_rmse, poly_lin_reg_mae, poly_lin_reg_r2))
print('Lasso Regression: RMSE = {:.3f}, MAE = {:.3f}, R^2 = {:.3f}'.format(lasso_reg_rmse, lasso_reg_mae, lasso_reg_r2))
print('Ridge Regression: RMSE = {:.3f}, MAE = {:.3f}, R^2 = {:.3f}'.format(ridge_reg_rmse, ridge_reg_mae, ridge_reg_r2))
print('MARS: RMSE = {:.3f}, MAE = {:.3f}, R^2 = {:.3f}'.format(mars_rmse, mars_mae, mars_r2))
